#!/usr/bin/env python
# coding: utf-8
'''
ì„œë²„ ì‹¤í–‰ì‹œ ì•„ë˜ ìš°ì„  ì‹¤í–‰ í•„ìˆ˜
pip install openpyxl FlagEmbedding chromadb
apt update && apt install zip -y

í´ë” ìƒì„± í›„ í´ë” ì••ì¶• ì½”ë“œ 
zip -r /root/chroma_db_bge.zip /root/chroma_db_bge
'''
import pandas as pd
import numpy as np
from tqdm import tqdm
import chromadb
from chromadb.config import Settings
from FlagEmbedding import BGEM3FlagModel

###############################
# 1) ChromaDB ì—°ê²°
###############################
# db í´ë” ì €ì¥í•˜ê³  ì‹¶ì€ ê²½ë¡œ
CHROMA_DB_DIR = "./chroma_db_bge"
client = chromadb.PersistentClient(path=CHROMA_DB_DIR)

# db collection ì´ë¦„ ì„¤ì • 
collection_name = "job_postings_collection"

try:
    # ì´ë¯¸ ì¡´ì¬í•˜ë©´ get_collection
    collection = client.get_collection(collection_name)
except:
    # ì—†ë‹¤ë©´ create_collection
    collection = client.create_collection(collection_name)

###############################
# 2) bge-m3 ëª¨ë¸ ë¡œë“œ
###############################
model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=False)

###############################
# 3) CSV(or XLSX) ë¡œë“œ
###############################
df = pd.read_excel("./all.xlsx")

###############################
# 4) ì„ë² ë”© ì¶”ì¶œí•  ì—´ ì •ì˜
###############################
COLUMNS_TO_EMBED = ["ê³µê³ ì œëª©", "ì£¼ìš”ì—…ë¬´", "ìê²©ìš”ê±´ë°ìš°ëŒ€ì‚¬í•­", "í˜œíƒë°ë³µì§€"]

###############################
# 5) Batch ì„ë² ë”© ì¤€ë¹„
###############################
texts = []
metadatas = []
doc_ids = []

for i, row in tqdm(df.iterrows(), total=len(df), desc="ì„ë² ë”© ì¤€ë¹„ ì¤‘", unit="row"):
    job_id = str(row["ê³µê³ id"])
    exp_val = row.get("ê²½ë ¥", 0)
    loc_val = row.get("ê·¼ë¬´ìœ„ì¹˜", "")

    for col_type in COLUMNS_TO_EMBED:
        text_data = str(row.get(col_type, "")).strip()
        if not text_data:
            text_data = " "

        # doc_id ì˜ˆ: "255120-ê³µê³ ì œëª©-0"
        doc_id = f"{job_id}-{col_type}-{i}"

        meta = {
            "ê³µê³ id": job_id,
            "type": col_type,
            "ê²½ë ¥": float(exp_val),
            "ê·¼ë¬´ìœ„ì¹˜": loc_val
        }

        texts.append(text_data)
        doc_ids.append(doc_id)
        metadatas.append(meta)

print(f"âœ… ì´ {len(texts)}ê°œ í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤€ë¹„ ì™„ë£Œ.")

###############################
# 6) Batch ì„ë² ë”© & upsert
###############################
if __name__ == "__main__":
    BATCH_SIZE = 3000
    ENCODE_BATCH_SIZE = 32

    total_docs = len(texts)
    for start_idx in range(0, total_docs, BATCH_SIZE):
        end_idx = start_idx + BATCH_SIZE
        batch_texts = texts[start_idx:end_idx]
        batch_ids = doc_ids[start_idx:end_idx]
        batch_metas = metadatas[start_idx:end_idx]

        # bge-m3 ì„ë² ë”©
        emb_out = model.encode(
            batch_texts,
            batch_size=ENCODE_BATCH_SIZE,
            max_length=8000,
            return_dense=True,
            return_sparse=False,
            return_colbert_vecs=False
        )
        batch_embeddings = emb_out["dense_vecs"]  # numpy array í˜•íƒœ

        # ChromaDB upsert
        # documentsì— ì‹¤ì œ í…ìŠ¤íŠ¸ë¥¼ ë„£ë„ë¡ ë³€ê²½
        collection.upsert(
            documents=batch_texts,
            embeddings=[emb.tolist() for emb in batch_embeddings],
            metadatas=batch_metas,
            ids=batch_ids
        )

        print(f"[{start_idx}~{end_idx-1}] batch upsert ì™„ë£Œ (ì´ {len(batch_texts)}ê±´)")

    print("âœ… ìµœì¢…ì ìœ¼ë¡œ ëª¨ë“  ë¬¸ì„œë¥¼ ChromaDBì— upsert ì™„ë£Œ!")

    # ğŸ”¥ ë©€í‹°í”„ë¡œì„¸ì‹± í’€ ì¢…ë£Œ(FlagEmbedding ê´€ë ¨)
    model.stop_self_pool()
